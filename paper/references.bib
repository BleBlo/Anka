% ============================================================================
% REFERENCES FOR ANKA PAPER
% ============================================================================

% --- LLM Code Generation ---

@article{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{nijkamp2023codegen,
  title={CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint arXiv:2203.13474},
  year={2023}
}

@article{li2023starcoder,
  title={StarCoder: May the Source Be With You!},
  author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023}
}

@misc{github2022copilot,
  title={GitHub Copilot: Your AI Pair Programmer},
  author={{GitHub}},
  year={2022},
  howpublished={\url{https://github.com/features/copilot}},
  note={Accessed: 2024-01-15}
}

@article{austin2021program,
  title={Program Synthesis with Large Language Models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{hendrycks2021measuring,
  title={Measuring Coding Challenge Competence With APPS},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  journal={arXiv preprint arXiv:2105.09938},
  year={2021}
}

% --- Code Generation Evaluation and Analysis ---

@inproceedings{pearce2022examining,
  title={Examining Zero-Shot Vulnerability Repair with Large Language Models},
  author={Pearce, Hammond and Tan, Benjamin and Ahmad, Baleegh and Karri, Ramesh and Dolan-Gavitt, Brendan},
  booktitle={2023 IEEE Symposium on Security and Privacy (SP)},
  pages={2339--2356},
  year={2023},
  organization={IEEE}
}

@article{jesse2023large,
  title={Large Language Models and Simple, Stupid Bugs},
  author={Jesse, Kevin and Toufique, Ahmed and Elbaum, Sebastian and Stolee, Kathryn T and Tip, Frank},
  journal={arXiv preprint arXiv:2303.11455},
  year={2023}
}

@article{zhang2023repocoder,
  title={RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation},
  author={Zhang, Fengji and Chen, Bei and Zhang, Yue and Liu, Jin and Zan, Daoguang and Mao, Yi and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2303.12570},
  year={2023}
}

@article{jimenez2024swebench,
  title={SWE-bench: Can Language Models Resolve Real-World GitHub Issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2310.06770},
  year={2024}
}

@article{yang2024sweagent,
  title={SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering},
  author={Yang, John and Jimenez, Carlos E and Wettig, Alexander and Liber, Kilian and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2405.15793},
  year={2024}
}

% --- Domain-Specific Languages ---

@book{fowler2010domain,
  title={Domain-Specific Languages},
  author={Fowler, Martin},
  year={2010},
  publisher={Pearson Education}
}

@article{mernik2005and,
  title={When and How to Develop Domain-Specific Languages},
  author={Mernik, Marjan and Heering, Jan and Sloane, Anthony M},
  journal={ACM Computing Surveys},
  volume={37},
  number={4},
  pages={316--344},
  year={2005},
  publisher={ACM}
}

@inproceedings{gulwani2011automating,
  title={Automating String Processing in Spreadsheets Using Input-Output Examples},
  author={Gulwani, Sumit},
  booktitle={Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  pages={317--330},
  year={2011}
}

@article{ellis2021dreamcoder,
  title={DreamCoder: Bootstrapping Inductive Program Synthesis with Wake-Sleep Library Learning},
  author={Ellis, Kevin and Wong, Catherine and Nye, Maxwell and Sable-Meyer, Mathias and Morales, Lucas and Hewitt, Luke and Cary, Luc and Solar-Lezama, Armando and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:2006.08381},
  year={2021}
}

% --- Prompt Engineering ---

@article{wei2022chain,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{wang2023selfconsistency,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2023}
}

@article{jiang2023selfplanning,
  title={Self-planning Code Generation with Large Language Models},
  author={Jiang, Xue and Dong, Yihong and Wang, Lecheng and Shang, Qiwei and Li, Ge},
  journal={arXiv preprint arXiv:2303.06689},
  year={2023}
}

% --- Constrained Generation ---

@inproceedings{scholak2021picard,
  title={PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models},
  author={Scholak, Torsten and Schucher, Nathan and Bahdanau, Dzmitry},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={9895--9901},
  year={2021}
}

@article{poesia2022synchromesh,
  title={Synchromesh: Reliable Code Generation from Pre-trained Language Models},
  author={Poesia, Gabriel and Polozov, Oleksandr and Le, Vu and Tiwari, Ashish and Soares, Gustavo and Meek, Christopher and Gulwani, Sumit},
  journal={arXiv preprint arXiv:2201.11227},
  year={2022}
}

% --- Parsing and Tools ---

@misc{lark2017,
  title={Lark: A Modern Parsing Library for Python},
  author={Shinan, Erez},
  year={2017},
  howpublished={\url{https://github.com/lark-parser/lark}},
  note={Accessed: 2024-01-15}
}

% --- Additional Code LLM Papers ---

@article{roziere2023code,
  title={Code Llama: Open Foundation Models for Code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{fried2023incoder,
  title={InCoder: A Generative Model for Code Infilling and Synthesis},
  author={Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:2204.05999},
  year={2023}
}

@article{wang2023codet5,
  title={CodeT5+: Open Code Large Language Models for Code Understanding and Generation},
  author={Wang, Yue and Le, Hung and Gotmare, Akhilesh Deepak and Bui, Nghi DQ and Li, Junnan and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2305.07922},
  year={2023}
}

% --- SQL and Data Languages ---

@article{zhong2017seq2sql,
  title={Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning},
  author={Zhong, Victor and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1709.00103},
  year={2017}
}

@inproceedings{yu2018spider,
  title={Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task},
  author={Yu, Tao and Zhang, Rui and Yang, Kai and Yasunaga, Michihiro and Wang, Dongxu and Li, Zifan and Ma, James and Li, Irene and Yao, Qingning and Roman, Shanelle and others},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={3911--3921},
  year={2018}
}

% --- Program Synthesis ---

@article{chen2019execution,
  title={Execution-Guided Neural Program Synthesis},
  author={Chen, Xinyun and Liu, Chang and Song, Dawn},
  journal={International Conference on Learning Representations},
  year={2019}
}

@article{devlin2017robustfill,
  title={RobustFill: Neural Program Learning under Noisy I/O},
  author={Devlin, Jacob and Uesato, Jonathan and Bhupatiraju, Surya and Singh, Rishabh and Mohamed, Abdel-rahman and Kohli, Pushmeet},
  journal={International Conference on Machine Learning},
  pages={990--998},
  year={2017}
}

% --- Analysis of LLM Capabilities ---

@article{bubeck2023sparks,
  title={Sparks of Artificial General Intelligence: Early Experiments with GPT-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
