\begin{thebibliography}{17}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, et~al.]{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{chen2021codex}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Ellis et~al.(2021)Ellis, Wong, Nye, Sable-Meyer, Morales, Hewitt, Cary, Solar-Lezama, and Tenenbaum]{ellis2021dreamcoder}
Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Lucas Morales, Luke Hewitt, Luc Cary, Armando Solar-Lezama, and Joshua~B Tenenbaum.
\newblock Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning.
\newblock \emph{arXiv preprint arXiv:2006.08381}, 2021.

\bibitem[Fowler(2010)]{fowler2010domain}
Martin Fowler.
\newblock \emph{Domain-Specific Languages}.
\newblock Pearson Education, 2010.

\bibitem[{GitHub}(2022)]{github2022copilot}
{GitHub}.
\newblock Github copilot: Your ai pair programmer.
\newblock \url{https://github.com/features/copilot}, 2022.
\newblock Accessed: 2024-01-15.

\bibitem[Gulwani(2011)]{gulwani2011automating}
Sumit Gulwani.
\newblock Automating string processing in spreadsheets using input-output examples.
\newblock In \emph{Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}, pages 317--330, 2011.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Basart, Kadavath, Mazeika, Arora, Guo, Burns, Puranik, He, Song, et~al.]{hendrycks2021measuring}
Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et~al.
\newblock Measuring coding challenge competence with apps.
\newblock \emph{arXiv preprint arXiv:2105.09938}, 2021.

\bibitem[Jesse et~al.(2023)Jesse, Toufique, Elbaum, Stolee, and Tip]{jesse2023large}
Kevin Jesse, Ahmed Toufique, Sebastian Elbaum, Kathryn~T Stolee, and Frank Tip.
\newblock Large language models and simple, stupid bugs.
\newblock \emph{arXiv preprint arXiv:2303.11455}, 2023.

\bibitem[Jiang et~al.(2023)Jiang, Dong, Wang, Shang, and Li]{jiang2023selfplanning}
Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, and Ge~Li.
\newblock Self-planning code generation with large language models.
\newblock \emph{arXiv preprint arXiv:2303.06689}, 2023.

\bibitem[Li et~al.(2023)Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim, et~al.]{li2023starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al.
\newblock Starcoder: May the source be with you!
\newblock \emph{arXiv preprint arXiv:2305.06161}, 2023.

\bibitem[Mernik et~al.(2005)Mernik, Heering, and Sloane]{mernik2005and}
Marjan Mernik, Jan Heering, and Anthony~M Sloane.
\newblock When and how to develop domain-specific languages.
\newblock \emph{ACM Computing Surveys}, 37\penalty0 (4):\penalty0 316--344, 2005.

\bibitem[Nijkamp et~al.(2023)Nijkamp, Pang, Hayashi, Tu, Wang, Zhou, Savarese, and Xiong]{nijkamp2023codegen}
Erik Nijkamp, Bo~Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.
\newblock Codegen: An open large language model for code with multi-turn program synthesis.
\newblock \emph{arXiv preprint arXiv:2203.13474}, 2023.

\bibitem[Pearce et~al.(2023)Pearce, Tan, Ahmad, Karri, and Dolan-Gavitt]{pearce2022examining}
Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan Dolan-Gavitt.
\newblock Examining zero-shot vulnerability repair with large language models.
\newblock In \emph{2023 IEEE Symposium on Security and Privacy (SP)}, pages 2339--2356. IEEE, 2023.

\bibitem[Poesia et~al.(2022)Poesia, Polozov, Le, Tiwari, Soares, Meek, and Gulwani]{poesia2022synchromesh}
Gabriel Poesia, Oleksandr Polozov, Vu~Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani.
\newblock Synchromesh: Reliable code generation from pre-trained language models.
\newblock \emph{arXiv preprint arXiv:2201.11227}, 2022.

\bibitem[Scholak et~al.(2021)Scholak, Schucher, and Bahdanau]{scholak2021picard}
Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau.
\newblock Picard: Parsing incrementally for constrained auto-regressive decoding from language models.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 9895--9901, 2021.

\bibitem[Wang et~al.(2023)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{wang2023selfconsistency}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock \emph{arXiv preprint arXiv:2203.11171}, 2023.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le, and Zhou]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed~Chi, Quoc Le, and Denny Zhou.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24824--24837, 2022.

\end{thebibliography}
