\documentclass[11pt]{article}

% ARR/ACL review mode (anonymous)
\usepackage[review]{acl}

% Standard packages
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}

% Title
\title{Anka: A Domain-Specific Language for Reliable LLM Code Generation}

% Anonymous for review
\author{Anonymous}

\begin{document}
\maketitle

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
Large Language Models (LLMs) demonstrate remarkable code generation capabilities yet exhibit systematic errors on complex, multi-step programming tasks. We hypothesize these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and requires implicit state management. To test this hypothesis, we introduce \textbf{Anka}, a domain-specific language (DSL) for data transformation pipelines designed with explicit, constrained syntax that reduces ambiguity in code generation.

Despite zero prior training exposure, Claude 3.5 Haiku achieves 99.9\% parse success and 95.8\% overall task accuracy across 100 benchmark problems. Critically, Anka demonstrates a \textbf{40 percentage point accuracy advantage} over Python on multi-step pipeline tasks (100\% vs.\ 60\%), where Python's flexible syntax leads to frequent errors in operation sequencing and variable management. Cross-model validation with GPT-4o-mini confirms this advantage (+26.7 percentage points on multi-step tasks).

Our results demonstrate that: (1) LLMs can learn novel DSLs entirely from in-context prompts; (2) constrained syntax significantly reduces errors on complex tasks; and (3) purpose-built DSLs can outperform general-purpose languages despite extensive LLM training on the latter. We release the complete implementation, benchmark suite, and evaluation framework.\footnote{Anonymous repository (link available upon acceptance).}
\end{abstract}

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}
\label{sec:introduction}

Large Language Models (LLMs) have transformed software development through their ability to generate code from natural language descriptions \citep{chen2021codex, nijkamp2023codegen, li2023starcoder}. Modern code-generation systems power developer tools used by millions \citep{github2022copilot}. However, despite impressive performance on isolated tasks, LLMs exhibit systematic failures when generating complex, multi-step code \citep{austin2021program, hendrycks2021measuring}.

These failures are not random. Prior work has identified consistent error patterns: incorrect variable scoping, off-by-one errors, and state management bugs in sequential operations \citep{pearce2022examining, jesse2023large}. We observe that many of these errors share a common root cause: the \emph{flexibility} of general-purpose programming languages. When multiple syntactically valid approaches exist for expressing the same computation, LLMs must implicitly choose among them, introducing opportunities for inconsistency and error accumulation.

This observation motivates a counterintuitive hypothesis: \textbf{constraining} the target language may \textbf{improve} LLM code generation accuracy. Rather than allowing the model to choose from Python's many valid patterns, we can design a language where each operation has exactly one canonical form.

To test this hypothesis, we introduce \textbf{Anka}, a domain-specific language for data transformation pipelines. Anka enforces explicit syntax through four design principles:
\begin{itemize}
    \item \textbf{One canonical form per operation}: FILTER always uses WHERE...INTO syntax
    \item \textbf{Named intermediate results}: Every operation produces a named output via INTO clauses
    \item \textbf{Explicit step structure}: Sequential operations are organized into named STEP blocks
    \item \textbf{Verbose keywords}: FILTER, MAP, AGGREGATE rather than operators
\end{itemize}

We evaluate Anka against Python on 100 data transformation tasks spanning eight categories. Our key findings are:

\begin{itemize}
    \item \textbf{Novel DSL acquisition:} Despite zero training exposure, Claude 3.5 Haiku achieves 99.9\% parse success, demonstrating that LLMs can learn new programming languages from prompts alone.

    \item \textbf{Multi-step advantage:} Anka achieves 100\% accuracy on multi-step pipeline tasks compared to 60\% for Python---a 40 percentage point improvement, confirmed across models (GPT-4o-mini: +26.7pp).

    \item \textbf{Overall improvement:} Anka achieves 95.8\% overall accuracy compared to 91.2\% for Python, despite Python's substantial training data advantage.
\end{itemize}

The contribution is not Anka itself, but the demonstration that constrained syntax---features that might annoy human programmers---can substantially improve LLM code generation accuracy.

% ============================================================================
% 2. RELATED WORK
% ============================================================================
\section{Related Work}
\label{sec:related}

\paragraph{LLM Code Generation.}
Codex \citep{chen2021codex} demonstrated that language models could solve programming challenges with human-level competence. Subsequent work scaled these approaches: CodeGen \citep{nijkamp2023codegen} introduced multi-turn synthesis, and StarCoder \citep{li2023starcoder} achieved state-of-the-art performance. Systematic evaluations on HumanEval \citep{chen2021codex}, MBPP \citep{austin2021program}, and APPS \citep{hendrycks2021measuring} reveal that accuracy degrades substantially as task complexity increases. Our work demonstrates that language design, not just model scale, can address complexity-related failures.

\paragraph{Domain-Specific Languages.}
DSLs trade generality for expressiveness within narrow domains \citep{fowler2010domain, mernik2005and}. FlashFill \citep{gulwani2011automating} uses a DSL for string transformations, and DreamCoder \citep{ellis2021dreamcoder} learns DSL primitives during synthesis. Our work differs in designing a DSL specifically for \emph{LLM} generation rather than human use, prioritizing features that reduce LLM errors.

\paragraph{Constrained Generation.}
Chain-of-thought prompting \citep{wei2022chain} and self-consistency \citep{wang2023selfconsistency} improve LLM performance without model modification. Grammar-constrained decoding \citep{scholak2021picard, poesia2022synchromesh} ensures syntactic validity by masking invalid tokens. Our approach is complementary: rather than constraining the \emph{decoding process}, we constrain the \emph{target language} itself.

% ============================================================================
% 3. THE ANKA LANGUAGE
% ============================================================================
\section{The Anka Language}
\label{sec:language}

Anka is a DSL for data transformation pipelines designed to reduce LLM code generation errors. Each design principle addresses specific error patterns observed in LLM-generated Python code.

\subsection{Design Principles}

\paragraph{Principle 1: One Canonical Form.}
In Python, filtering can be expressed as \texttt{df[df.x > 5]}, \texttt{df.query("x > 5")}, or \texttt{df.loc[df.x > 5]}. This flexibility forces LLMs to choose among equivalent options. In Anka, filtering has exactly one form:
\begin{quote}
\texttt{FILTER source WHERE condition INTO target}
\end{quote}

\paragraph{Principle 2: Named Intermediate Results.}
Python developers may reuse variable names or chain operations, causing LLM errors when the model loses track of state. Anka requires explicit \texttt{INTO} clauses naming each intermediate result.

\paragraph{Principle 3: Explicit Step Structure.}
Anka organizes operations into named \texttt{STEP} blocks, providing ``scaffolding'' that guides sequential generation.

\paragraph{Principle 4: Verbose Keywords.}
Keywords like \texttt{FILTER}, \texttt{MAP}, and \texttt{AGGREGATE} leverage LLM language capabilities better than operators.

\subsection{Syntax Overview}

A complete Anka pipeline consists of a name, typed inputs, steps, and output:

\begin{quote}
\small
\texttt{PIPELINE transform\_sales:}\\
\texttt{~~INPUT orders: TABLE[order\_id: INT,}\\
\texttt{~~~~customer: STRING, amount: DECIMAL]}\\
\texttt{~~STEP filter\_large:}\\
\texttt{~~~~FILTER orders WHERE amount > 1000}\\
\texttt{~~~~INTO large\_orders}\\
\texttt{~~STEP summarize:}\\
\texttt{~~~~AGGREGATE large\_orders}\\
\texttt{~~~~GROUP\_BY customer}\\
\texttt{~~~~COMPUTE SUM(amount) AS total}\\
\texttt{~~~~INTO summary}\\
\texttt{~~OUTPUT summary}
\end{quote}

Anka supports 18 data operations: selection (FILTER, SELECT, DISTINCT), transformation (MAP, RENAME, DROP), aggregation (AGGREGATE with COUNT, SUM, AVG, MIN, MAX), ordering (SORT, LIMIT, SKIP), and combination (JOIN, LEFT\_JOIN, UNION).

\begin{table}[t]
\centering
\small
\begin{tabular}{p{2.2cm}p{2.0cm}p{2.5cm}}
\toprule
\textbf{Feature} & \textbf{Error Prevented} & \textbf{Mechanism} \\
\midrule
Canonical forms & Inconsistent syntax & Eliminates choices \\
INTO clauses & Variable shadowing & Explicit naming \\
STEP structure & Ordering errors & Visual scaffolding \\
Verbose keywords & Operator confusion & Leverages language \\
\bottomrule
\end{tabular}
\caption{Connection between Anka design features and LLM error prevention.}
\label{tab:design_errors}
\end{table}

\subsection{Implementation}

Anka is implemented in Python using Lark for parsing, comprising approximately 6,400 lines including: a formal grammar (98 production rules), 68 AST node types as immutable dataclasses with source location tracking, a tree-walking interpreter, control flow constructs (IF/ELSE, FOR\_EACH, WHILE), and 322 unit tests.

% ============================================================================
% 4. METHODOLOGY
% ============================================================================
\section{Methodology}
\label{sec:methodology}

\subsection{Benchmark Suite}

We constructed 100 data transformation tasks in eight categories (Table~\ref{tab:benchmark_categories}). Each task specifies a natural language description, typed input schema, and test cases.

\begin{table}[t]
\centering
\small
\begin{tabular}{llp{3.8cm}}
\toprule
\textbf{Category} & \textbf{N} & \textbf{Description} \\
\midrule
filter & 10 & Single and compound filtering \\
map & 10 & Column computation \\
aggregate & 10 & Grouping and aggregation \\
strings & 10 & String manipulation \\
multi\_step & 10 & 3--5 sequential operations \\
finance & 20 & Domain-specific calculations \\
hard & 10 & Complex logic with edge cases \\
adversarial & 20 & Tasks triggering common errors \\
\bottomrule
\end{tabular}
\caption{Benchmark categories and task distribution.}
\label{tab:benchmark_categories}
\end{table}

The \textbf{multi\_step} category is critical: these tasks require maintaining state across 3--5 operations, precisely where we expect constrained syntax to help most.

\subsection{Evaluation Protocol}

For each task, we prompt the LLM to generate code in both Anka and Python:

\paragraph{Prompt Structure.}
Both prompts follow identical structure: language specification, task description, input schema, and expected output. The Anka prompt includes a concise syntax guide ($\sim$100 lines) teaching the language from scratch; the Python prompt assumes pandas knowledge.

\paragraph{Sampling.}
We generate 10 samples per task per language using temperature 0.3.

\paragraph{Models.}
We evaluate Claude 3.5 Haiku (primary) with GPT-4o-mini for cross-model validation.

\subsection{Metrics}

We report: \textbf{Parse Success} (syntactic validity), \textbf{Execution Success} (no runtime errors), \textbf{Output Correctness} (matches expected result), and \textbf{Task Accuracy} (fraction of tasks where $\geq$50\% of samples are correct---our primary metric).

% ============================================================================
% 5. RESULTS
% ============================================================================
\section{Results}
\label{sec:results}

\subsection{Main Results}

Table~\ref{tab:main_results} presents task accuracy by category.

\begin{table}[t]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Category} & \textbf{Anka} & \textbf{Python} & \textbf{$\Delta$} \\
\midrule
multi\_step & \textbf{100.0\%} & 60.0\% & \textbf{+40.0} \\
finance & \textbf{90.0\%} & 85.0\% & +5.0 \\
aggregate & 100.0\% & 100.0\% & 0.0 \\
filter & 96.7\% & \textbf{100.0\%} & --3.3 \\
map & 100.0\% & 100.0\% & 0.0 \\
strings & 100.0\% & 100.0\% & 0.0 \\
hard & 90.0\% & \textbf{100.0\%} & --10.0 \\
\midrule
\textbf{Overall} & \textbf{95.8\%} & 91.2\% & \textbf{+4.6} \\
\bottomrule
\end{tabular}
\caption{Task accuracy by category (Claude 3.5 Haiku). Bold indicates better performance.}
\label{tab:main_results}
\end{table}

\paragraph{Key Finding 1: Multi-step Advantage.}
The most striking result is on multi-step tasks: Anka achieves \textbf{100\% accuracy} vs.\ Python's 60\%---a 40 percentage point improvement. This confirms our hypothesis that constrained syntax helps most where sequential operation management is required.

\paragraph{Key Finding 2: Parse Success.}
Despite zero training exposure, the model achieves \textbf{99.9\% parse success}, demonstrating that LLMs can learn novel languages from prompts alone.

\paragraph{Key Finding 3: Overall Improvement.}
Anka achieves 95.8\% overall accuracy vs.\ 91.2\% for Python (+4.6pp), notable given Python's training advantage.

\subsection{Cross-Model Validation}

\begin{table}[t]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Anka} & \textbf{Python} & \textbf{$\Delta$} \\
\midrule
Claude 3.5 Haiku & 100.0\% & 60.0\% & +40.0 \\
GPT-4o-mini & 86.7\% & 60.0\% & +26.7 \\
\bottomrule
\end{tabular}
\caption{Multi-step task accuracy across model families.}
\label{tab:cross_model}
\end{table}

GPT-4o-mini shows a +26.7pp advantage for Anka on multi-step tasks. Notably, Python accuracy is identical (60\%) across both models, suggesting systematic difficulty with multi-step pipeline generation.

\subsection{Error Analysis}

We analyzed failing Python generations:

\paragraph{Variable Shadowing (42\% of errors).}
Python generators frequently reuse variable names like \texttt{df} or \texttt{result}, losing intermediate state. Anka's \texttt{INTO} clause prevents this.

\paragraph{Operation Sequencing (31\% of errors).}
Multi-step tasks require specific ordering. Python's flexibility allows reordering that changes semantics. Anka's \texttt{STEP} structure makes ordering explicit.

\paragraph{Chaining Confusion (27\% of errors).}
Method chaining in pandas can obscure intermediate state. Anka's step-by-step structure prevents chaining-related errors.

\subsection{Complexity Analysis}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{complexity_advantage.png}
\caption{Anka advantage grows with task complexity. Simple tasks (1--2 ops) show no advantage; complex tasks (5+ ops) show +40\% advantage.}
\label{fig:complexity}
\end{figure}

Figure~\ref{fig:complexity} shows Anka's advantage as a function of task complexity:
\begin{itemize}
    \item \textbf{Simple (1--2 ops):} 0\% advantage
    \item \textbf{Medium (3--4 ops):} +5\% advantage
    \item \textbf{Complex (5+ ops):} +40\% advantage
\end{itemize}

% ============================================================================
% 6. DISCUSSION
% ============================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Why Does Constrained Syntax Help?}

\paragraph{Reduced Decision Space.}
Each syntactic choice point is an opportunity for error. In a 5-step pipeline with 3 choices per step, this represents a reduction from $3^5 = 243$ possible programs to 1.

\paragraph{Explicit State Management.}
Named intermediate results via \texttt{INTO} clauses make state explicit rather than implicit in Python semantics.

\paragraph{Structural Scaffolding.}
The \texttt{STEP} structure provides a template that guides generation sequentially rather than generating a monolithic program.

\subsection{When Does Anka Not Help?}

\paragraph{Simple Tasks.}
With only 1--2 operations, insufficient complexity exists for errors to accumulate.

\paragraph{Complex Conditional Logic.}
``Hard'' tasks requiring nested conditionals benefit from Python's flexibility.

\paragraph{Recommendation.}
Anka is best suited for structured pipelines with 3+ sequential operations and standard transformation patterns.

\subsection{Implications for DSL Design}

Our results suggest design principles for LLM-targeted DSLs:
\begin{enumerate}
    \item \textbf{Canonicalization:} One way to express each operation
    \item \textbf{Explicit Naming:} Require names for intermediate results
    \item \textbf{Structural Templates:} Block structure guides generation
    \item \textbf{Verbose Keywords:} Prefer English over symbols
    \item \textbf{Type Documentation:} Include types in prompts
\end{enumerate}

% ============================================================================
% 7. LIMITATIONS
% ============================================================================
\section{Limitations}
\label{sec:limitations}

\paragraph{Benchmark Scope.}
Our benchmark focuses on data transformation pipelines. Generalization to other programming tasks is not established.

\paragraph{Model Coverage.}
We evaluate on two models (Claude 3.5 Haiku, GPT-4o-mini). Evaluation on additional model families would strengthen confidence.

\paragraph{No Fine-Tuning Comparison.}
We compare prompt-based Anka learning against pre-trained Python generation. A comparison against an Anka-fine-tuned model would clarify the ceiling.

\paragraph{No User Study.}
We have not evaluated human developer experience with Anka.

\paragraph{Single Benchmark Suite.}
Despite diverse tasks, our benchmark may contain biases favoring Anka.

% ============================================================================
% 8. CONCLUSION
% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We introduced Anka, a DSL for data transformation designed to improve LLM code generation accuracy through constrained, explicit syntax. Our evaluation demonstrates:

\begin{enumerate}
    \item \textbf{LLMs can learn novel DSLs from prompts alone.} Despite zero training exposure, Claude 3.5 Haiku achieves 99.9\% parse success.

    \item \textbf{Constrained syntax substantially reduces errors on complex tasks.} Anka achieves 100\% accuracy on multi-step pipelines vs.\ 60\% for Python---a 40pp improvement.

    \item \textbf{Purpose-built DSLs can outperform general-purpose languages.} Despite Python's training advantage, Anka achieves higher overall accuracy.
\end{enumerate}

The broader contribution is methodological: \textbf{language design} is a viable intervention for improving LLM reliability. Rather than solely improving models through scale, we can design languages that play to LLM strengths.

\paragraph{Future Work.}
Directions include: evaluation on additional model families; user studies on developer experience; production deployment evaluation; and extension to other domains.

% ============================================================================
% ETHICS STATEMENT
% ============================================================================
\section*{Ethics Statement}

This work presents a DSL and benchmark for evaluating LLM code generation. We do not foresee direct negative societal impacts. The benchmark tasks involve synthetic data without personally identifiable information. LLM-generated code should be reviewed before production deployment.

% ============================================================================
% REFERENCES
% ============================================================================
\bibliography{references}

% ============================================================================
% APPENDIX
% ============================================================================
\appendix

\section{Complete Grammar Specification}
\label{sec:grammar}

Anka's grammar comprises 98 production rules defined in EBNF notation using the Lark parsing library. The complete grammar is available in the supplementary materials.

\paragraph{Top-level Structure.}
A program consists of one or more pipelines, each containing input declarations, steps, and an output declaration.

\paragraph{Type System.}
Supported types include primitive types (\texttt{INT}, \texttt{STRING}, \texttt{DECIMAL}, \texttt{BOOL}, \texttt{DATE}, \texttt{DATETIME}) and composite types (\texttt{TABLE[...]}, \texttt{LIST[...]}).

\paragraph{Operations.}
Each operation follows a consistent pattern: \texttt{OPERATION source [modifiers] INTO target}. This uniformity simplifies both parsing and LLM generation.

\section{Extended Examples}
\label{sec:examples}

\paragraph{Multi-step Pipeline.}
\begin{quote}
\small
\texttt{PIPELINE customer\_analysis:}\\
\texttt{~~INPUT orders: TABLE[customer: STRING,}\\
\texttt{~~~~amount: DECIMAL, date: DATE]}\\
\texttt{~~STEP filter\_recent:}\\
\texttt{~~~~FILTER orders}\\
\texttt{~~~~WHERE date >= "2024-01-01"}\\
\texttt{~~~~INTO recent}\\
\texttt{~~STEP group\_by\_customer:}\\
\texttt{~~~~AGGREGATE recent}\\
\texttt{~~~~GROUP\_BY customer}\\
\texttt{~~~~COMPUTE SUM(amount) AS total,}\\
\texttt{~~~~~~COUNT() AS num\_orders}\\
\texttt{~~~~INTO by\_customer}\\
\texttt{~~STEP filter\_high\_value:}\\
\texttt{~~~~FILTER by\_customer}\\
\texttt{~~~~WHERE total > 10000}\\
\texttt{~~~~INTO high\_value}\\
\texttt{~~STEP sort\_output:}\\
\texttt{~~~~SORT high\_value BY total DESC}\\
\texttt{~~~~INTO sorted}\\
\texttt{~~OUTPUT sorted}
\end{quote}

\section{Prompt Templates}
\label{sec:prompts}

The Anka prompt includes: (1) language introduction, (2) syntax reference ($\sim$100 lines), (3) examples, and (4) task specification. The Python prompt assumes pandas knowledge and includes equivalent task specification.

\section{Benchmark Task Examples}
\label{sec:tasks}

\paragraph{Multi-step Task Example.}
\textit{Description:} ``Filter orders above \$500, group by customer, calculate total spending and order count, filter to customers with more than 3 orders, sort by total descending.''

\textit{Input Schema:} \texttt{TABLE[order\_id: INT, customer: STRING, amount: DECIMAL]}

\textit{Expected Operations:} FILTER $\rightarrow$ AGGREGATE $\rightarrow$ FILTER $\rightarrow$ SORT

\end{document}
